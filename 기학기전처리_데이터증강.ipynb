{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaegoori/handsignproject/blob/datapreprocessing/%EA%B8%B0%ED%95%99%EA%B8%B0%EC%A0%84%EC%B2%98%EB%A6%AC_%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%A6%9D%EA%B0%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qyke1kXWVMFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe4b6535-d1ac-4213-d059-1af662bfcbd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Drive 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeOczp1sOgMJ",
        "outputId": "cb001dd3-92e0-47b2-ef6d-78d5e1937af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is terminated.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "SRC=\"/content/drive/MyDrive/keypoints_all\"\n",
        "DST=\"/content/drive/MyDrive/keypoints_160F\"\n",
        "\n",
        "mkdir -p \"$DST\"\n",
        "\n",
        "echo \"정확한 WORD0001~0160 + REAL01~16 + F 버전만 추출 시작\"\n",
        "\n",
        "for z in \"$SRC\"/*.zip; do\n",
        "    echo \"ZIP 처리 중: $z\"\n",
        "\n",
        "    # 정확한 숫자 범위만 포함 (0001~0160)\n",
        "    for i in $(seq -f \"%04g\" 1 160); do\n",
        "        unzip -qq \"$z\" \"*/NIA_SL_WORD${i}_REAL*_F*\" -d \"$DST\"\n",
        "    done\n",
        "done\n",
        "\n",
        "echo \"추출 완료\"\n",
        "echo \"저장 위치: $DST\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/01_real_word_morpheme.zip\"   # 네가 업로드한 경로\n",
        "extract_to = \"/content/drive/MyDrive/morpheme_full\"\n",
        "\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "with ZipFile(zip_path, 'r') as zf:\n",
        "    zf.extractall(extract_to)\n",
        "\n",
        "print(\"압축해제 완료:\", extract_to)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2Mw-1Ef1A2P",
        "outputId": "5e13f912-eeb5-468b-a1a1-0e083772e9f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "압축해제 완료: /content/drive/MyDrive/morpheme_full\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XueAiHhh9wxH",
        "outputId": "28e437ae-0a28-4923-bcd9-dbc5503d1b3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "선택된 단어 수: 160개 (160개면 정상)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Morpheme 파일 로드: 100%|██████████| 159672/159672 [00:00<00:00, 353926.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "필터링된 morpheme (REAL ID 기준) 개수: 1760개\n",
            "선택된 keypoint 폴더 수: 2560개\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "단어 매칭 중: 100%|██████████| 2560/2560 [00:00<00:00, 105017.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최종 매칭된 데이터 개수: 1760개\n",
            "\n",
            " 저장 완료: /content/drive/MyDrive/sign_data/fast_index/fast_match.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os, json, glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 0. 경로 설정\n",
        "# Keypoint 폴더의 루트 경로\n",
        "keypoint_root = \"/content/drive/MyDrive/keypoints_160F\"\n",
        "# Morpheme 파일의 루트 경로\n",
        "morpheme_root = \"/content/drive/MyDrive/morpheme_full/morpheme\"\n",
        "# 최종 JSON 파일이 저장될 경로\n",
        "output_dir = \"/content/drive/MyDrive/sign_data/fast_index\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ============================\n",
        "# 1. WORD 0001 ~ 0160 자동 생성\n",
        "# ============================\n",
        "selected_words = {f\"NIA_SL_WORD{str(i).zfill(4)}\" for i in range(1, 161)}\n",
        "print(f\"선택된 단어 수: {len(selected_words)}개 (160개면 정상)\")\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 2. morpheme 파일 로드 및 REAL ID 딕셔너리 생성\n",
        "# (NIA_SL_WORD0001_REAL02 → 파일 경로)\n",
        "# ============================\n",
        "morpheme_files = glob.glob(os.path.join(morpheme_root, \"**/*.json\"), recursive=True)\n",
        "morpheme_dict = {}\n",
        "\n",
        "def extract_real_id_from_morpheme(filename):\n",
        "    # NIA_SL_WORD0001_REAL02_morpheme.json → NIA_SL_WORD0001_REAL02 추출\n",
        "    name = os.path.basename(filename).replace(\"_morpheme.json\", \"\")\n",
        "    return name.rsplit(\"_\", 1)[0]\n",
        "\n",
        "for f in tqdm(morpheme_files, desc=\"Morpheme 파일 로드\"):\n",
        "    real_id = extract_real_id_from_morpheme(f)\n",
        "    base_word = real_id.split(\"_REAL\")[0] # NIA_SL_WORD0001\n",
        "\n",
        "    # 선택된 단어에 포함되는 경우에만 딕셔너리에 추가\n",
        "    if base_word in selected_words:\n",
        "        # REAL ID를 고유 키로 사용하여 1:1 매칭 준비\n",
        "        morpheme_dict[real_id] = f\n",
        "\n",
        "print(f\"필터링된 morpheme (REAL ID 기준) 개수: {len(morpheme_dict)}개\")\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 3. keypoint 폴더 로드 (F 버전만)\n",
        "# ============================\n",
        "keypoint_dirs = []\n",
        "\n",
        "for root, dirs, files in os.walk(keypoint_root):\n",
        "    for d in dirs:\n",
        "        # 예: NIA_SL_WORD0001_REAL03_F 형식의 폴더만 선택\n",
        "        if d.startswith(\"NIA_SL_WORD\") and \"_REAL\" in d and d.endswith(\"_F\"):\n",
        "            keypoint_dirs.append(os.path.join(root, d))\n",
        "\n",
        "print(f\"선택된 keypoint 폴더 수: {len(keypoint_dirs)}개\")\n",
        "\n",
        "\n",
        "def extract_real_id_from_kp(folder_name):\n",
        "    # NIA_SL_WORD0001_REAL03_F → NIA_SL_WORD0001_REAL03 추출\n",
        "    return folder_name.rsplit(\"_\", 1)[0]\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 4. keypoint ↔ morpheme 1:1 매칭 및 fast_index 생성\n",
        "# ============================\n",
        "fast_index = []\n",
        "\n",
        "for kp_path in tqdm(keypoint_dirs, desc=\"단어 매칭 중\"):\n",
        "    folder_name = os.path.basename(kp_path)\n",
        "\n",
        "    # 1:1 매칭을 위한 REAL ID 추출\n",
        "    real_id = extract_real_id_from_kp(folder_name)\n",
        "\n",
        "    # base_word 추출 (선택된 단어 확인용)\n",
        "    base_word = real_id.split(\"_REAL\")[0]\n",
        "\n",
        "    # base_word가 선택 목록에 있고, 해당 REAL ID의 morpheme 파일이 morpheme_dict에 있는 경우 매칭\n",
        "    if base_word in selected_words and real_id in morpheme_dict:\n",
        "        morpheme_path = morpheme_dict[real_id]\n",
        "\n",
        "        fast_index.append({\n",
        "            \"word_id\": base_word,\n",
        "            \"real_id\": real_id, # 매칭된 고유 ID\n",
        "            \"keypoint_path\": kp_path,\n",
        "            \"morpheme_path\": morpheme_path\n",
        "        })\n",
        "\n",
        "print(f\"최종 매칭된 데이터 개수: {len(fast_index)}개\")\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 5. JSON 저장\n",
        "# ============================\n",
        "output_path = os.path.join(output_dir, \"fast_match.json\")\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(fast_index, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(\"\\n 저장 완료:\", output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e_IrMCVWhLj",
        "outputId": "809c10b0-6006-4c4a-fa02-4550c2ed42c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ujson\n",
            "  Downloading ujson-5.11.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (9.4 kB)\n",
            "Downloading ujson-5.11.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (57 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ujson\n",
            "Successfully installed ujson-5.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ujson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfGGReu7CzSl",
        "outputId": "aef7b0d9-481d-429a-ff80-bbcbca3ddc56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "처리할 매칭 쌍 개수: 1760개\n",
            "사용 가능한 CPU 코어 수: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "데이터 결합 중: 100%|██████████| 1760/1760 [10:22:26<00:00, 21.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "데이터를 JSON 파일로 저장 중...\n",
            "\n",
            "✅ 완료\n",
            "최종 저장된 형태소 세그먼트 개수: 1757개\n",
            "경로: /content/drive/MyDrive/sign_data/processed/merged_dataset.json\n"
          ]
        }
      ],
      "source": [
        "import os, json, ujson, numpy as np\n",
        "from tqdm import tqdm\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "# ============================\n",
        "# 0. 경로 설정\n",
        "# ============================\n",
        "# 입력 파일: 이전 단계에서 생성된 1:1 매칭 인덱스 파일\n",
        "fast_index_path = \"/content/drive/MyDrive/sign_data/fast_index/fast_match.json\"\n",
        "# 출력 파일: Keypoint와 형태소 정보를 결합한 최종 데이터셋\n",
        "output_path = \"/content/drive/MyDrive/sign_data/processed/merged_dataset.json\"\n",
        "\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "# 매칭 인덱스 로드\n",
        "try:\n",
        "    with open(fast_index_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        pairs = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    print(f\"오류: fast_index 파일이 경로에 없습니다. 경로를 확인해주세요: {fast_index_path}\")\n",
        "    pairs = []\n",
        "except Exception as e:\n",
        "    print(f\"오류: fast_index 파일 로드 중 예외 발생: {e}\")\n",
        "    pairs = []\n",
        "\n",
        "print(f\"처리할 매칭 쌍 개수: {len(pairs)}개\")\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 1. Keypoint 데이터 로드 및 정제 함수\n",
        "# ============================\n",
        "def load_person_data(fp):\n",
        "    \"\"\"프레임 하나 빠르게 읽고 (N, 3) Keypoint 배열 반환\"\"\"\n",
        "    try:\n",
        "        # ujson으로 파일 빠르게 로드\n",
        "        with open(fp, \"r\") as f:\n",
        "            kf = ujson.load(f)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "    # 'people' 데이터 추출 및 형식 통일\n",
        "    people = kf.get(\"people\", [])\n",
        "    if isinstance(people, dict):\n",
        "        people = [people.get(\"0\", people)]\n",
        "\n",
        "    if not people:\n",
        "        return None\n",
        "\n",
        "    person = people[0]\n",
        "\n",
        "    def safe_array(key):\n",
        "        \"\"\"키포인트를 안전하게 로드하고 (N, 3) 형태로 변환\"\"\"\n",
        "        arr = person.get(key, [])\n",
        "        if not arr:\n",
        "            # 데이터가 없는 경우 (0, 3) 배열 반환\n",
        "            return np.zeros((0, 3), dtype=np.float32)\n",
        "\n",
        "        arr = np.array(arr, dtype=np.float32)\n",
        "        # 2D 키포인트는 (N*3,) 형태이므로 (N, 3)으로 재구성\n",
        "        return arr.reshape(-1, 3)\n",
        "\n",
        "    # Keypoint 추출 및 연결 순서: Pose(25) + LeftHand(21) + RightHand(21) + Face(70) = 137 Keypoints\n",
        "    p = safe_array(\"pose_keypoints_2d\")\n",
        "    lh = safe_array(\"hand_left_keypoints_2d\")\n",
        "    rh = safe_array(\"hand_right_keypoints_2d\")\n",
        "    face = safe_array(\"face_keypoints_2d\")\n",
        "\n",
        "    # 모든 키포인트 연결\n",
        "    full = np.concatenate([p, lh, rh, face], axis=0)\n",
        "\n",
        "    # 신뢰도(Confidence, 3번째 열)가 0.5 미만인 키포인트는 0으로 설정\n",
        "    full[full[:, 2] < 0.5] = 0\n",
        "\n",
        "    return full\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 2. 병렬 처리 함수\n",
        "# ============================\n",
        "def process_item(item):\n",
        "    \"\"\"단어 1개의 Keypoint와 Morpheme을 결합하여 프레임 시퀀스 생성\"\"\"\n",
        "    keypoint_dir = item[\"keypoint_path\"]\n",
        "    morpheme_path = item[\"morpheme_path\"]\n",
        "    fps = 30 # 초당 프레임 수 (NIA 수어 데이터셋 표준)\n",
        "    real_id = item.get(\"real_id\", \"N/A\") # real_id 가져오기\n",
        "\n",
        "    # Morpheme 로드\n",
        "    try:\n",
        "        with open(morpheme_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            mor = ujson.load(f)\n",
        "        segments = mor.get(\"data\", [])\n",
        "        if not isinstance(segments, list):\n",
        "             segments = [segments]\n",
        "    except Exception:\n",
        "        # print(f\"Morpheme 로드 오류: {morpheme_path}\")\n",
        "        return []\n",
        "\n",
        "    # Keypoint 프레임 파일 목록\n",
        "    frame_files = sorted([\n",
        "        os.path.join(keypoint_dir, f)\n",
        "        for f in os.listdir(keypoint_dir)\n",
        "        if f.endswith(\".json\")\n",
        "    ])\n",
        "\n",
        "    if not frame_files:\n",
        "        return []\n",
        "\n",
        "    # 모든 프레임 데이터 로드\n",
        "    frames = []\n",
        "    for fp in frame_files:\n",
        "        full = load_person_data(fp)\n",
        "        # 키포인트 개수 확인 (137개 = 25+21+21+70)\n",
        "        if full is not None and full.shape[0] == 137:\n",
        "            frames.append(full)\n",
        "\n",
        "    if not frames:\n",
        "        return []\n",
        "\n",
        "    frames = np.stack(frames).astype(np.float32)\n",
        "    results = []\n",
        "\n",
        "    # 형태소 구간별 잘라서 저장\n",
        "    for seg in segments:\n",
        "        try:\n",
        "            start_idx = int(seg[\"start\"] * fps)\n",
        "            end_idx = int(seg[\"end\"] * fps)\n",
        "\n",
        "            # 유효성 검사\n",
        "            if start_idx >= len(frames) or end_idx <= start_idx:\n",
        "                continue\n",
        "\n",
        "            # 프레임 자르기\n",
        "            cut = frames[start_idx:min(end_idx, len(frames))]\n",
        "\n",
        "            results.append({\n",
        "                \"word_id\": item[\"word_id\"],\n",
        "                \"real_id\": real_id,\n",
        "                \"label\": seg[\"attributes\"], # 형태소 레이블\n",
        "                \"frames\": cut.tolist() # 넘파이 배열을 리스트로 변환\n",
        "            })\n",
        "        except Exception:\n",
        "             # print(f\"세그먼트 처리 오류: {real_id} - {seg}\")\n",
        "             continue\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 3. 단어 전체를 병렬 처리 및 데이터 병합\n",
        "# ============================\n",
        "merged_data = []\n",
        "if pairs:\n",
        "    # CPU 코어 수만큼 Pool 생성\n",
        "    core_count = cpu_count()\n",
        "    print(f\"사용 가능한 CPU 코어 수: {core_count}\")\n",
        "    with Pool(core_count) as P:\n",
        "        # imap_unordered를 사용하여 결과를 비동기적으로 가져옴\n",
        "        for res in tqdm(P.imap_unordered(process_item, pairs), total=len(pairs), desc=\"데이터 결합 중\"):\n",
        "            merged_data.extend(res)\n",
        "\n",
        "# ============================\n",
        "# 4. JSON 저장 (ujson 사용)\n",
        "# ============================\n",
        "if merged_data:\n",
        "    try:\n",
        "        print(\"\\n데이터를 JSON 파일로 저장 중...\")\n",
        "        with open(output_path, \"w\") as f:\n",
        "            ujson.dump(merged_data, f)\n",
        "\n",
        "        print(\"\\n 완료\")\n",
        "        print(f\"최종 저장된 형태소 세그먼트 개수: {len(merged_data)}개\")\n",
        "        print(f\"경로: {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"저장 오류: {e}\")\n",
        "else:\n",
        "    print(\"\\n 처리된 데이터가 없어 저장하지 않습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVNHrJ6GCtAp",
        "outputId": "a599b2b0-2a24-4fa9-d43d-5cd28991d5bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 로드 시작...\n",
            "로드 완료. 총 세그먼트 수: 1757\n",
            "첫 샘플 키: ['word_id', 'real_id', 'label', 'frames']\n"
          ]
        }
      ],
      "source": [
        "# 0. JSON 로딩 + 기본 구조 확인\n",
        "import numpy as np\n",
        "import json\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. 파일 경로\n",
        "file_path = '/content/drive/MyDrive/sign_data/processed/merged_dataset.json'\n",
        "\n",
        "print(\"데이터 로드 시작...\")\n",
        "with open(file_path, 'r') as f:\n",
        "    data_json = json.load(f)\n",
        "print(\"로드 완료. 총 세그먼트 수:\", len(data_json))\n",
        "\n",
        "# 첫 항목 구조 확인\n",
        "print(\"첫 샘플 키:\", list(data_json[0].keys()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLSF-7ZYCz9J",
        "outputId": "709a9533-fc36-4a9e-d23e-4cfb8e1f046b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "frames 추출 시작...\n",
            "총 1757개 샘플 padding 중...\n",
            "padding 완료. raw_data shape: (1757, 106, 137, 3)\n"
          ]
        }
      ],
      "source": [
        "# 1. frames 데이터만 추출 → padding → raw_data 만들기\n",
        "print(\"frames 추출 시작...\")\n",
        "\n",
        "keypoint_data_list = [item[\"frames\"] for item in data_json]\n",
        "\n",
        "print(f\"총 {len(keypoint_data_list)}개 샘플 padding 중...\")\n",
        "\n",
        "padded_data = pad_sequences(keypoint_data_list, padding='post', dtype='float32')\n",
        "\n",
        "raw_data = np.array(padded_data)\n",
        "\n",
        "print(\"padding 완료. raw_data shape:\", raw_data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7pRis2aC3tO",
        "outputId": "3a757ce2-3ac2-4969-9537-e59a52d725db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reshape 완료 → (1757, 106, 411)\n"
          ]
        }
      ],
      "source": [
        "# 2. raw_data 4D → (N,T,F*3) 형태로 변환\n",
        "# raw_data: (N, T, 137, 3)\n",
        "\n",
        "try:\n",
        "    N, T, P, C = raw_data.shape  # P=137, C=3\n",
        "    F = P * C\n",
        "    raw_data = raw_data.reshape(N, T, F)\n",
        "    print(\"reshape 완료 →\", raw_data.shape)\n",
        "except:\n",
        "    print(\"이미 (N,T,F) 형태일 가능성 있음.\")\n",
        "    raw_data = raw_data  # 그대로 사용\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrginJLjC7nQ",
        "outputId": "89a3acfc-a050-46e1-956a-8505da541402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mid-Hip 상대좌표 변환 중...\n",
            "완료: (1757, 106, 411)\n"
          ]
        }
      ],
      "source": [
        "# 3. Mid-Hip 기준 상대좌표 변환\n",
        "def apply_midhip_relative(raw):\n",
        "    print(\"Mid-Hip 상대좌표 변환 중...\")\n",
        "    N, T, F = raw.shape\n",
        "    num_points = F // 3\n",
        "    REF = 0  # Mid-Hip index\n",
        "\n",
        "    ref_point = raw[:, :, REF*3:(REF+1)*3]\n",
        "    ref_point_expand = np.tile(ref_point, (1,1,num_points))\n",
        "\n",
        "    rel = raw - ref_point_expand\n",
        "    print(\"완료:\", rel.shape)\n",
        "    return rel\n",
        "\n",
        "X_midhip = apply_midhip_relative(raw_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f85EGqCgnDyB",
        "outputId": "2f59e82d-5426-4855-d0cb-4fffeb9f67c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "핵심 keypoint 선택 중...\n",
            "완료: (1757, 106, 54)\n"
          ]
        }
      ],
      "source": [
        "# 4. 핵심 Keypoint 선택 (총 18개 → 54차원)\n",
        "POSE_IDXS = [0,1,2,3,4,5,6,7]\n",
        "HAND_IDXS = [4,8,12,16,20]\n",
        "\n",
        "def select_keypoints(frame_137x3):\n",
        "    pose = frame_137x3[:25]\n",
        "    lh   = frame_137x3[25:46]\n",
        "    rh   = frame_137x3[46:67]\n",
        "\n",
        "    selected = []\n",
        "    for idx in POSE_IDXS: selected.append(pose[idx])\n",
        "    for idx in HAND_IDXS: selected.append(lh[idx])\n",
        "    for idx in HAND_IDXS: selected.append(rh[idx])\n",
        "\n",
        "    selected = np.array(selected)\n",
        "    return selected.reshape(-1)  # 54\n",
        "\n",
        "def apply_keypoint_selection(X):\n",
        "    print(\"핵심 keypoint 선택 중...\")\n",
        "    N,T,F = X.shape\n",
        "    num_points = F // 3\n",
        "    X_reshaped = X.reshape(N,T,num_points,3)\n",
        "\n",
        "    X_new = []\n",
        "    for i in range(N):\n",
        "        seq_new = []\n",
        "        for t in range(T):\n",
        "            seq_new.append(select_keypoints(X_reshaped[i,t]))\n",
        "        X_new.append(seq_new)\n",
        "\n",
        "    print(\"완료:\", np.array(X_new).shape)\n",
        "    return np.array(X_new)\n",
        "\n",
        "X_selected = apply_keypoint_selection(X_midhip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30mEtLqx3ZjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83612caf-d542-4d22-ea1f-b02404111ade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "보간 시작...\n",
            "보간 완료: (1757, 106, 54)\n"
          ]
        }
      ],
      "source": [
        "# 5. confidence 0 구간 interpolation\n",
        "def interpolate_low_confidence(frames, th=0.5):\n",
        "    T,F = frames.shape\n",
        "    P = F//3\n",
        "    arr = frames.reshape(T,P,3)\n",
        "\n",
        "    for p in range(P):\n",
        "        for d in range(2):\n",
        "            coords = arr[:,p,d]\n",
        "            conf   = arr[:,p,2]\n",
        "\n",
        "            low = np.where(conf < th)[0]\n",
        "            for idx in low:\n",
        "                prev = idx-1\n",
        "                next = idx+1\n",
        "                while prev>=0 and arr[prev,p,2]<th: prev-=1\n",
        "                while next<T and arr[next,p,2]<th: next+=1\n",
        "                if prev<0 or next>=T: continue\n",
        "                arr[idx,p,d] = (arr[prev,p,d]+arr[next,p,d])/2\n",
        "                arr[idx,p,2] = max(arr[prev,p,2], arr[next,p,2])\n",
        "\n",
        "    return arr.reshape(T,F)\n",
        "\n",
        "print(\"보간 시작...\")\n",
        "X_interp = np.zeros_like(X_selected)\n",
        "for i in range(len(X_selected)):\n",
        "    X_interp[i] = interpolate_low_confidence(X_selected[i])\n",
        "print(\"보간 완료:\", X_interp.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojKCv1kd3gor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b53ce0cd-1f39-4f79-a18e-ce0d640d9cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 클래스: 163\n"
          ]
        }
      ],
      "source": [
        "# 6. Label 인코딩\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labels_text = np.array([item[\"label\"][0][\"name\"] for item in data_json])\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(labels_text)\n",
        "\n",
        "print(\"총 클래스:\", len(encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bByTLK5Q3l7d",
        "outputId": "129511bb-e954-4a6f-99f6-45544af146d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (1405, 106, 54)\n",
            "Test : (352, 106, 54)\n"
          ]
        }
      ],
      "source": [
        "# 7. Train/Test split (stratify 없음)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_interp, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(\"Train:\", X_train.shape)\n",
        "print(\"Test :\", X_test.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik6lXLeT8Hze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9051c45b-ece0-42d7-b640-1c00ae57a012"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "스케일링 완료\n"
          ]
        }
      ],
      "source": [
        "# 8. StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def scale_per_feature(X_train, X_test):\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # F(특징) 기준 scaling\n",
        "    X_train_scaled = scaler.fit_transform(\n",
        "        X_train.reshape(-1, X_train.shape[-1])\n",
        "    ).reshape(X_train.shape)\n",
        "\n",
        "    X_test_scaled = scaler.transform(\n",
        "        X_test.reshape(-1, X_test.shape[-1])\n",
        "    ).reshape(X_test.shape)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, scaler\n",
        "\n",
        "X_train_scaled, X_test_scaled, scaler = scale_per_feature(X_train, X_test)\n",
        "print(\"스케일링 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1tf7WAm8I5T",
        "outputId": "44ad15e7-5d8c-4f42-ccd6-46d98298fc35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 준비 완료\n"
          ]
        }
      ],
      "source": [
        "# 모델 학습 준비 완료\n",
        "X_train_final = X_train_scaled\n",
        "X_test_final = X_test_scaled\n",
        "y_train_final = y_train\n",
        "y_test_final = y_test\n",
        "\n",
        "print(\"학습 준비 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_path, 'r') as f:\n",
        "    data_json = json.load(f)\n",
        "\n",
        "print(\"로드 완료. 총 세그먼트 수:\", len(data_json))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7FkPdIvGaS4",
        "outputId": "25966e3f-1786-458a-d165-7964dddafa84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "로드 완료. 총 세그먼트 수: 1757\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}