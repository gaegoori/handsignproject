{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaegoori/handsignproject/blob/datapreprocessing/%EA%B8%B0%ED%95%99%EA%B8%B0%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qyke1kXWVMFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed67174-2109-4100-8527-d6769f70fde5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Drive 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeOczp1sOgMJ",
        "outputId": "cb001dd3-92e0-47b2-ef6d-78d5e1937af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is terminated.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "SRC=\"/content/drive/MyDrive/keypoints_all\"\n",
        "DST=\"/content/drive/MyDrive/keypoints_160F\"\n",
        "\n",
        "mkdir -p \"$DST\"\n",
        "\n",
        "echo \"정확한 WORD0001~0160 + REAL01~16 + F 버전만 추출 시작\"\n",
        "\n",
        "for z in \"$SRC\"/*.zip; do\n",
        "    echo \"ZIP 처리 중: $z\"\n",
        "\n",
        "    # word 숫자 범위만 포함 (0001~0160)\n",
        "    for i in $(seq -f \"%04g\" 1 160); do\n",
        "        unzip -qq \"$z\" \"*/NIA_SL_WORD${i}_REAL*_F*\" -d \"$DST\"\n",
        "    done\n",
        "done\n",
        "\n",
        "echo \"추출 완료\"\n",
        "echo \"저장 위치: $DST\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/01_real_word_morpheme.zip\"\n",
        "extract_to = \"/content/drive/MyDrive/morpheme_full\"\n",
        "\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "with ZipFile(zip_path, 'r') as zf:\n",
        "    zf.extractall(extract_to)\n",
        "\n",
        "print(\"압축해제 완료:\", extract_to)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2Mw-1Ef1A2P",
        "outputId": "5e13f912-eeb5-468b-a1a1-0e083772e9f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "압축해제 완료: /content/drive/MyDrive/morpheme_full\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XueAiHhh9wxH",
        "outputId": "28e437ae-0a28-4923-bcd9-dbc5503d1b3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "선택된 단어 수: 160개 (160개면 정상)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Morpheme 파일 로드: 100%|██████████| 159672/159672 [00:00<00:00, 353926.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "필터링된 morpheme (REAL ID 기준) 개수: 1760개\n",
            "선택된 keypoint 폴더 수: 2560개\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "단어 매칭 중: 100%|██████████| 2560/2560 [00:00<00:00, 105017.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최종 매칭된 데이터 개수: 1760개\n",
            "\n",
            " 저장 완료: /content/drive/MyDrive/sign_data/fast_index/fast_match.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os, json, glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 0. 경로 설정\n",
        "# Keypoint 폴더의 루트 경로\n",
        "keypoint_root = \"/content/drive/MyDrive/keypoints_160F\"\n",
        "# Morpheme 파일의 루트 경로\n",
        "morpheme_root = \"/content/drive/MyDrive/morpheme_full/morpheme\"\n",
        "# 최종 JSON 파일이 저장될 경로\n",
        "output_dir = \"/content/drive/MyDrive/sign_data/fast_index\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# 1. WORD 0001 ~ 0160 생성\n",
        "selected_words = {f\"NIA_SL_WORD{str(i).zfill(4)}\" for i in range(1, 161)}\n",
        "print(f\"선택된 단어 수: {len(selected_words)}개 \")\n",
        "\n",
        "\n",
        "\n",
        "# 2. morpheme 파일 로드 및 REAL ID 딕셔너리 생성\n",
        "morpheme_files = glob.glob(os.path.join(morpheme_root, \"**/*.json\"), recursive=True)\n",
        "morpheme_dict = {}\n",
        "\n",
        "def extract_real_id_from_morpheme(filename):\n",
        "    # NIA_SL_WORD0001_REAL02_morpheme.json → NIA_SL_WORD0001_REAL02 추출\n",
        "    name = os.path.basename(filename).replace(\"_morpheme.json\", \"\")\n",
        "    return name.rsplit(\"_\", 1)[0]\n",
        "\n",
        "for f in tqdm(morpheme_files, desc=\"Morpheme 파일 로드\"):\n",
        "    real_id = extract_real_id_from_morpheme(f)\n",
        "    base_word = real_id.split(\"_REAL\")[0] # NIA_SL_WORD0001\n",
        "\n",
        "    # 선택된 단어에 포함되는 경우에만 딕셔너리에 추가\n",
        "    if base_word in selected_words:\n",
        "        # REAL ID를 고유 키로 사용하여 1:1 매칭 준비\n",
        "        morpheme_dict[real_id] = f\n",
        "\n",
        "print(f\"필터링된 morpheme (REAL ID 기준) 개수: {len(morpheme_dict)}개\")\n",
        "\n",
        "\n",
        "# 3. keypoint 폴더 로드 (F 측면)\n",
        "keypoint_dirs = []\n",
        "\n",
        "for root, dirs, files in os.walk(keypoint_root):\n",
        "    for d in dirs:\n",
        "        # 예: NIA_SL_WORD0001_REAL03_F 형식의 폴더만 선택\n",
        "        if d.startswith(\"NIA_SL_WORD\") and \"_REAL\" in d and d.endswith(\"_F\"):\n",
        "            keypoint_dirs.append(os.path.join(root, d))\n",
        "\n",
        "print(f\"선택된 keypoint 폴더 수: {len(keypoint_dirs)}개\")\n",
        "\n",
        "\n",
        "def extract_real_id_from_kp(folder_name):\n",
        "    # NIA_SL_WORD0001_REAL03_F → NIA_SL_WORD0001_REAL03 추출\n",
        "    return folder_name.rsplit(\"_\", 1)[0]\n",
        "\n",
        "\n",
        "# 4. keypoint ↔ morpheme 1:1 매칭 및 fast_index 생성\n",
        "fast_index = []\n",
        "\n",
        "for kp_path in tqdm(keypoint_dirs, desc=\"단어 매칭 중\"):\n",
        "    folder_name = os.path.basename(kp_path)\n",
        "\n",
        "    # 1:1 매칭을 위한 REAL ID 추출\n",
        "    real_id = extract_real_id_from_kp(folder_name)\n",
        "\n",
        "    # base_word 추출 (선택된 단어 확인용)\n",
        "    base_word = real_id.split(\"_REAL\")[0]\n",
        "\n",
        "    # base_word가 선택 목록에 있고, 해당 REAL ID의 morpheme 파일이 morpheme_dict에 있는 경우 매칭\n",
        "    if base_word in selected_words and real_id in morpheme_dict:\n",
        "        morpheme_path = morpheme_dict[real_id]\n",
        "\n",
        "        fast_index.append({\n",
        "            \"word_id\": base_word,\n",
        "            \"real_id\": real_id, # 매칭된 고유 ID\n",
        "            \"keypoint_path\": kp_path,\n",
        "            \"morpheme_path\": morpheme_path\n",
        "        })\n",
        "\n",
        "print(f\"최종 매칭된 데이터 개수: {len(fast_index)}개\")\n",
        "\n",
        "\n",
        "\n",
        "# 5. JSON 저장\n",
        "output_path = os.path.join(output_dir, \"fast_match.json\")\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(fast_index, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(\"\\n 저장 완료:\", output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e_IrMCVWhLj",
        "outputId": "809c10b0-6006-4c4a-fa02-4550c2ed42c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ujson\n",
            "  Downloading ujson-5.11.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (9.4 kB)\n",
            "Downloading ujson-5.11.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (57 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ujson\n",
            "Successfully installed ujson-5.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ujson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfGGReu7CzSl",
        "outputId": "aef7b0d9-481d-429a-ff80-bbcbca3ddc56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "처리할 매칭 쌍 개수: 1760개\n",
            "사용 가능한 CPU 코어 수: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "데이터 결합 중: 100%|██████████| 1760/1760 [10:22:26<00:00, 21.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "데이터를 JSON 파일로 저장 중...\n",
            "\n",
            "✅ 완료\n",
            "최종 저장된 형태소 세그먼트 개수: 1757개\n",
            "경로: /content/drive/MyDrive/sign_data/processed/merged_dataset.json\n"
          ]
        }
      ],
      "source": [
        "import os, json, ujson, numpy as np\n",
        "from tqdm import tqdm\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "# 0. 경로 설정\n",
        "fast_index_path = \"/content/drive/MyDrive/sign_data/fast_index/fast_match.json\"\n",
        "output_path = \"/content/drive/MyDrive/sign_data/processed/merged_dataset.json\"\n",
        "\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "# 매칭 인덱스 처리\n",
        "try:\n",
        "    with open(fast_index_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        pairs = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    print(f\"오류: fast_index 파일이 경로에 없습니다. 경로를 확인해주세요: {fast_index_path}\")\n",
        "    pairs = []\n",
        "except Exception as e:\n",
        "    print(f\"오류: fast_index 파일 로드 중 예외 발생: {e}\")\n",
        "    pairs = []\n",
        "\n",
        "print(f\"처리할 매칭 쌍 개수: {len(pairs)}개\")\n",
        "\n",
        "\n",
        "# 1. Keypoint 데이터 로드 및 정제 함수\n",
        "def load_person_data(fp):\n",
        "    try:\n",
        "        with open(fp, \"r\") as f:\n",
        "            kf = ujson.load(f)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "    people = kf.get(\"people\", [])\n",
        "    if isinstance(people, dict):\n",
        "        people = [people.get(\"0\", people)]\n",
        "\n",
        "    if not people:\n",
        "        return None\n",
        "\n",
        "    person = people[0]\n",
        "\n",
        "    def safe_array(key):\n",
        "        arr = person.get(key, [])\n",
        "        if not arr:\n",
        "            return np.zeros((0, 3), dtype=np.float32)\n",
        "\n",
        "        arr = np.array(arr, dtype=np.float32)\n",
        "        return arr.reshape(-1, 3)\n",
        "\n",
        "    # Keypoint 추출 및 연결 순서: Pose(25) + LeftHand(21) + RightHand(21) + Face(70) = 137 Keypoints\n",
        "    p = safe_array(\"pose_keypoints_2d\")\n",
        "    lh = safe_array(\"hand_left_keypoints_2d\")\n",
        "    rh = safe_array(\"hand_right_keypoints_2d\")\n",
        "    face = safe_array(\"face_keypoints_2d\")\n",
        "\n",
        "    # 모든 키포인트 연결\n",
        "    full = np.concatenate([p, lh, rh, face], axis=0)\n",
        "\n",
        "    return full\n",
        "\n",
        "\n",
        "\n",
        "# 2. 병렬 처리 함수\n",
        "def process_item(item):\n",
        "    keypoint_dir = item[\"keypoint_path\"]\n",
        "    morpheme_path = item[\"morpheme_path\"]\n",
        "    fps = 30 # 초당 프레임 수 (NIA 수어 데이터셋 표준)\n",
        "    real_id = item.get(\"real_id\", \"N/A\")\n",
        "\n",
        "    # Morpheme\n",
        "    try:\n",
        "        with open(morpheme_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            mor = ujson.load(f)\n",
        "        segments = mor.get(\"data\", [])\n",
        "        if not isinstance(segments, list):\n",
        "             segments = [segments]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "    # Keypoint\n",
        "    frame_files = sorted([\n",
        "        os.path.join(keypoint_dir, f)\n",
        "        for f in os.listdir(keypoint_dir)\n",
        "        if f.endswith(\".json\")\n",
        "    ])\n",
        "\n",
        "    if not frame_files:\n",
        "        return []\n",
        "\n",
        "    # 모든 프레임 데이터 로드\n",
        "    frames = []\n",
        "    for fp in frame_files:\n",
        "        full = load_person_data(fp)\n",
        "        # 키포인트 개수 확인 (137개 = 25+21+21+70)\n",
        "        if full is not None and full.shape[0] == 137:\n",
        "            frames.append(full)\n",
        "\n",
        "    if not frames:\n",
        "        return []\n",
        "\n",
        "    frames = np.stack(frames).astype(np.float32)\n",
        "    results = []\n",
        "\n",
        "    # 형태소 구간별 잘라서 저장\n",
        "    for seg in segments:\n",
        "        try:\n",
        "            start_idx = int(seg[\"start\"] * fps)\n",
        "            end_idx = int(seg[\"end\"] * fps)\n",
        "\n",
        "            # 유효성 검사\n",
        "            if start_idx >= len(frames) or end_idx <= start_idx:\n",
        "                continue\n",
        "\n",
        "            # 프레임 자르기\n",
        "            cut = frames[start_idx:min(end_idx, len(frames))]\n",
        "\n",
        "            results.append({\n",
        "                \"word_id\": item[\"word_id\"],\n",
        "                \"real_id\": real_id,\n",
        "                \"label\": seg[\"attributes\"], # 형태소 레이블\n",
        "                \"frames\": cut.tolist() # 넘파이 배열을 리스트로 변환\n",
        "            })\n",
        "        except Exception:\n",
        "             continue\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# 3. 단어 전체를 병렬 처리 및 데이터 병합\n",
        "merged_data = []\n",
        "if pairs:\n",
        "    # CPU 코어 수만큼 Pool 생성\n",
        "    core_count = cpu_count()\n",
        "    print(f\"사용 가능한 CPU 코어 수: {core_count}\")\n",
        "    with Pool(core_count) as P:\n",
        "        for res in tqdm(P.imap_unordered(process_item, pairs), total=len(pairs), desc=\"데이터 결합 중\"):\n",
        "            merged_data.extend(res)\n",
        "\n",
        "\n",
        "# 4. JSON 저장 (ujson 사용)\n",
        "if merged_data:\n",
        "    try:\n",
        "        print(\"\\n데이터를 JSON 파일로 저장 중...\")\n",
        "        with open(output_path, \"w\") as f:\n",
        "            ujson.dump(merged_data, f)\n",
        "\n",
        "        print(\"\\n 완료\")\n",
        "        print(f\"최종 저장된 형태소 세그먼트 개수: {len(merged_data)}개\")\n",
        "        print(f\"경로: {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"저장 오류: {e}\")\n",
        "else:\n",
        "    print(\"\\n 처리된 데이터가 없어 저장하지 않습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. 파일 로드\n",
        "file_path = '/content/drive/MyDrive/sign_data/processed/merged_dataset.json'\n",
        "\n",
        "print(\"데이터 로드...\")\n",
        "with open(file_path, 'r') as f:\n",
        "    data_json = json.load(f)\n",
        "print(\"로드 완료. 총 세그먼트 수:\", len(data_json))\n",
        "\n",
        "\n",
        "# 2. 라벨 오타 수정 + 샘플 2개 미만 라벨 제거\n",
        "label_fix_map = {\n",
        "    \"꽈베기\": \"꽈배기\",\n",
        "    \"배추국\": \"배춧국\",\n",
        "    \"된장찌게\": \"된장찌개\",\n",
        "    \"결승전1\": \"결승전\",\n",
        "}\n",
        "\n",
        "data_fixed = []\n",
        "for item in data_json:\n",
        "    name = item['label'][0]['name']\n",
        "    item['label'][0]['name'] = label_fix_map.get(name, name)\n",
        "    data_fixed.append(item)\n",
        "\n",
        "# 빈도 세기\n",
        "labels_all = [item['label'][0]['name'] for item in data_fixed]\n",
        "freq = Counter(labels_all)\n",
        "remove_labels = [k for k, v in freq.items() if v < 2]\n",
        "\n",
        "print(\"제거할 라벨들:\", remove_labels)\n",
        "\n",
        "# 필터링된 최종 데이터\n",
        "filtered = [\n",
        "    item for item in data_fixed\n",
        "    if item['label'][0]['name'] not in remove_labels\n",
        "]\n",
        "\n",
        "print(\"정제 후 세그먼트 수:\", len(filtered))\n",
        "\n",
        "\n",
        "# 3. frames padding → raw_data 생성\n",
        "keypoint_data_list = [item[\"frames\"] for item in filtered]\n",
        "\n",
        "print(\"padding 중...\")\n",
        "padded_data = pad_sequences(keypoint_data_list, padding='post', dtype='float32')\n",
        "raw_data = np.array(padded_data)\n",
        "print(\"raw_data shape:\", raw_data.shape)   # (N, T, 137, 3)\n",
        "\n",
        "\n",
        "# 4. 4D → (N,T,F) reshape\n",
        "N, T, P, C = raw_data.shape\n",
        "F = P * C\n",
        "raw_flat = raw_data.reshape(N, T, F)\n",
        "print(\"reshape 완료:\", raw_flat.shape)\n",
        "\n",
        "\n",
        "# 5. Mid-Hip 상대좌표\n",
        "def apply_midhip_relative(raw):\n",
        "    print(\"Mid-Hip 상대좌표 변환 중...\")\n",
        "\n",
        "    N, T, F = raw.shape\n",
        "    num_points = F // 3\n",
        "\n",
        "    arr = raw.reshape(N, T, num_points, 3)\n",
        "\n",
        "    REF = 0  # Mid-Hip index\n",
        "    ref_xy = arr[:, :, REF, :2]      # (N,T,2)\n",
        "    ref_xy = ref_xy[:, :, None, :]   # (N,T,1,2)\n",
        "\n",
        "    arr[:, :, :, :2] -= ref_xy       # x,y만 기준점 빼기\n",
        "\n",
        "    return arr.reshape(N, T, F)\n",
        "\n",
        "X_midhip = apply_midhip_relative(raw_flat)\n",
        "print(\"Mid-Hip 완료:\", X_midhip.shape)\n",
        "\n",
        "\n",
        "# 6. 핵심 keypoint 선택 (18개 → 54차원: 18 * (x,y,conf))\n",
        "POSE_IDXS = [0,1,2,3,4,5,6,7]\n",
        "HAND_IDXS = [4,8,12,16,20]\n",
        "\n",
        "def select_keypoints(frame_137x3):\n",
        "    pose = frame_137x3[:25]\n",
        "    lh   = frame_137x3[25:46]\n",
        "    rh   = frame_137x3[46:67]\n",
        "\n",
        "    selected = []\n",
        "    for idx in POSE_IDXS: selected.append(pose[idx])\n",
        "    for idx in HAND_IDXS: selected.append(lh[idx])\n",
        "    for idx in HAND_IDXS: selected.append(rh[idx])\n",
        "\n",
        "    return np.array(selected).reshape(-1)   # (18*3,)\n",
        "\n",
        "\n",
        "def apply_keypoint_selection(X):\n",
        "    print(\"핵심 keypoint 선택 중...\")\n",
        "    N, T, F = X.shape\n",
        "    num_points = F // 3\n",
        "    Xr = X.reshape(N, T, num_points, 3)\n",
        "\n",
        "    X_new = np.zeros((N, T, 54), dtype=np.float32)\n",
        "\n",
        "    for i in range(N):\n",
        "        for t in range(T):\n",
        "            X_new[i, t] = select_keypoints(Xr[i, t])\n",
        "\n",
        "    print(\"완료:\", X_new.shape)\n",
        "    return X_new\n",
        "\n",
        "X_selected = apply_keypoint_selection(X_midhip)\n",
        "\n",
        "\n",
        "# 7. 강화된 interpolation (v2)\n",
        "def interpolate_strong(frames, th=0.5):\n",
        "    T, F = frames.shape\n",
        "    P = F // 3\n",
        "    arr = frames.reshape(T, P, 3)\n",
        "\n",
        "    for p in range(P):  # 각 keypoint마다 처리\n",
        "        conf = arr[:, p, 2]\n",
        "        x = arr[:, p, 0]\n",
        "        y = arr[:, p, 1]\n",
        "\n",
        "        # 1) forward fill\n",
        "        for t in range(1, T):\n",
        "            if conf[t] < th and conf[t-1] >= th:\n",
        "                x[t] = x[t-1]\n",
        "                y[t] = y[t-1]\n",
        "                conf[t] = conf[t-1]\n",
        "\n",
        "        # 2) backward fill\n",
        "        for t in range(T-2, -1, -1):\n",
        "            if conf[t] < th and conf[t+1] >= th:\n",
        "                x[t] = x[t+1]\n",
        "                y[t] = y[t+1]\n",
        "                conf[t] = conf[t+1]\n",
        "\n",
        "        # 3) segment-based interpolation\n",
        "        low_idx = np.where(conf < th)[0]\n",
        "\n",
        "        if len(low_idx) > 0:\n",
        "            seg_starts = []\n",
        "            seg_ends = []\n",
        "\n",
        "            start = low_idx[0]\n",
        "            prev = low_idx[0]\n",
        "\n",
        "            for idx in low_idx[1:]:\n",
        "                if idx != prev + 1:\n",
        "                    seg_starts.append(start)\n",
        "                    seg_ends.append(prev)\n",
        "                    start = idx\n",
        "                prev = idx\n",
        "            seg_starts.append(start)\n",
        "            seg_ends.append(prev)\n",
        "\n",
        "            # 각 연속 구간 보간\n",
        "            for s, e in zip(seg_starts, seg_ends):\n",
        "                prev_t = s - 1\n",
        "                next_t = e + 1\n",
        "\n",
        "                if prev_t < 0 or next_t >= T:\n",
        "                    continue\n",
        "\n",
        "                for t in range(s, e+1):\n",
        "                    ratio = (t - s + 1) / (e - s + 2)\n",
        "                    x[t] = (1-ratio) * x[prev_t] + ratio * x[next_t]\n",
        "                    y[t] = (1-ratio) * y[prev_t] + ratio * y[next_t]\n",
        "                    conf[t] = max(conf[prev_t], conf[next_t])\n",
        "\n",
        "        arr[:, p, 0] = x\n",
        "        arr[:, p, 1] = y\n",
        "        arr[:, p, 2] = conf\n",
        "\n",
        "    return arr.reshape(T, F)\n",
        "\n",
        "\n",
        "print(\"interpolation v2 적용 중...\")\n",
        "X_interp = np.zeros_like(X_selected)\n",
        "for i in range(len(X_selected)):\n",
        "    X_interp[i] = interpolate_strong(X_selected[i])\n",
        "print(\"보간 완료:\", X_interp.shape)\n",
        "\n",
        "\n",
        "# 8. confidence 제거 (x,y만)\n",
        "def remove_confidence(X):\n",
        "    N, T, F = X.shape\n",
        "    P = F // 3\n",
        "    coords = X.reshape(N, T, P, 3)[:, :, :, :2]   # (N,T,P,2)\n",
        "    return coords.reshape(N, T, P*2)              # (N,T,36)\n",
        "\n",
        "X_xy = remove_confidence(X_interp)\n",
        "print(\"confidence 제거 완료:\", X_xy.shape)\n",
        "\n",
        "\n",
        "# 9. 단순 스케일링\n",
        "X_xy /= 1000.0\n",
        "\n",
        "\n",
        "# 10. Train/Test split (⚠ filtered 기준으로 라벨 생성)\n",
        "labels_text = np.array([item[\"label\"][0][\"name\"] for item in filtered])\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(labels_text)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_xy, y,\n",
        "    test_size=0.2,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train:\", X_train.shape)\n",
        "print(\"Test :\", X_test.shape)\n",
        "print(\"클래스 개수:\", len(encoder.classes_))\n",
        "print(\"전처리 파이프라인 완료 v\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1gXjLYAKpcS",
        "outputId": "1c8a1af2-8251-4eb9-e32e-30ac5ac3615f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 로드...\n",
            "로드 완료. 총 세그먼트 수: 1757\n",
            "제거할 라벨들: []\n",
            "정제 후 세그먼트 수: 1757\n",
            "padding 중...\n",
            "raw_data shape: (1757, 106, 137, 3)\n",
            "reshape 완료: (1757, 106, 411)\n",
            "Mid-Hip 상대좌표 변환 중...\n",
            "Mid-Hip 완료: (1757, 106, 411)\n",
            "핵심 keypoint 선택 중...\n",
            "완료: (1757, 106, 54)\n",
            "interpolation v2 적용 중...\n",
            "보간 완료: (1757, 106, 54)\n",
            "confidence 제거 완료: (1757, 106, 36)\n",
            "Train: (1405, 106, 36)\n",
            "Test : (352, 106, 36)\n",
            "클래스 개수: 159\n",
            "전처리 파이프라인 완료 v\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W0Vet2WNijph"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}