{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaegoori/handsignproject/blob/model/%EA%B8%B0%ED%95%99%EA%B8%B0%EC%A0%84%EC%B2%98%EB%A6%AC_%EB%AA%A8%EB%8D%B8(final).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qyke1kXWVMFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc2c9b03-43f8-4d7e-9013-41357e839296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Drive ë§ˆìš´íŠ¸\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeOczp1sOgMJ"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "SRC=\"/content/drive/MyDrive/keypoints_all\"\n",
        "DST=\"/content/drive/MyDrive/keypoints_160F\"\n",
        "\n",
        "mkdir -p \"$DST\"\n",
        "\n",
        "echo \"ì •í™•í•œ WORD0001~0160 + REAL01~16 + F ë²„ì „ë§Œ ì¶”ì¶œ ì‹œì‘\"\n",
        "\n",
        "for z in \"$SRC\"/*.zip; do\n",
        "    echo \"ZIP ì²˜ë¦¬ ì¤‘: $z\"\n",
        "\n",
        "    # ì •í™•í•œ ìˆ«ì ë²”ìœ„ë§Œ í¬í•¨ (0001~0160)\n",
        "    for i in $(seq -f \"%04g\" 1 160); do\n",
        "        unzip -qq \"$z\" \"*/NIA_SL_WORD${i}_REAL*_F*\" -d \"$DST\"\n",
        "    done\n",
        "done\n",
        "\n",
        "echo \"ì¶”ì¶œ ì™„ë£Œ\"\n",
        "echo \"ì €ì¥ ìœ„ì¹˜: $DST\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/01_real_word_morpheme.zip\"   # ë„¤ê°€ ì—…ë¡œë“œí•œ ê²½ë¡œ\n",
        "extract_to = \"/content/drive/MyDrive/morpheme_full\"\n",
        "\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "with ZipFile(zip_path, 'r') as zf:\n",
        "    zf.extractall(extract_to)\n",
        "\n",
        "print(\"ì••ì¶•í•´ì œ ì™„ë£Œ:\", extract_to)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2Mw-1Ef1A2P",
        "outputId": "5e13f912-eeb5-468b-a1a1-0e083772e9f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì••ì¶•í•´ì œ ì™„ë£Œ: /content/drive/MyDrive/morpheme_full\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XueAiHhh9wxH",
        "outputId": "28e437ae-0a28-4923-bcd9-dbc5503d1b3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì„ íƒëœ ë‹¨ì–´ ìˆ˜: 160ê°œ (160ê°œë©´ ì •ìƒ)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Morpheme íŒŒì¼ ë¡œë“œ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 159672/159672 [00:00<00:00, 353926.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "í•„í„°ë§ëœ morpheme (REAL ID ê¸°ì¤€) ê°œìˆ˜: 1760ê°œ\n",
            "ì„ íƒëœ keypoint í´ë” ìˆ˜: 2560ê°œ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ë‹¨ì–´ ë§¤ì¹­ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2560/2560 [00:00<00:00, 105017.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìµœì¢… ë§¤ì¹­ëœ ë°ì´í„° ê°œìˆ˜: 1760ê°œ\n",
            "\n",
            " ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/sign_data/fast_index/fast_match.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os, json, glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 0. ê²½ë¡œ ì„¤ì •\n",
        "# Keypoint í´ë”ì˜ ë£¨íŠ¸ ê²½ë¡œ\n",
        "keypoint_root = \"/content/drive/MyDrive/keypoints_160F\"\n",
        "# Morpheme íŒŒì¼ì˜ ë£¨íŠ¸ ê²½ë¡œ\n",
        "morpheme_root = \"/content/drive/MyDrive/morpheme_full/morpheme\"\n",
        "# ìµœì¢… JSON íŒŒì¼ì´ ì €ì¥ë  ê²½ë¡œ\n",
        "output_dir = \"/content/drive/MyDrive/sign_data/fast_index\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ============================\n",
        "# 1. WORD 0001 ~ 0160 ìë™ ìƒì„±\n",
        "# ============================\n",
        "selected_words = {f\"NIA_SL_WORD{str(i).zfill(4)}\" for i in range(1, 161)}\n",
        "print(f\"ì„ íƒëœ ë‹¨ì–´ ìˆ˜: {len(selected_words)}ê°œ (160ê°œë©´ ì •ìƒ)\")\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 2. morpheme íŒŒì¼ ë¡œë“œ ë° REAL ID ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
        "# (NIA_SL_WORD0001_REAL02 â†’ íŒŒì¼ ê²½ë¡œ)\n",
        "# ============================\n",
        "morpheme_files = glob.glob(os.path.join(morpheme_root, \"**/*.json\"), recursive=True)\n",
        "morpheme_dict = {}\n",
        "\n",
        "def extract_real_id_from_morpheme(filename):\n",
        "    # NIA_SL_WORD0001_REAL02_morpheme.json â†’ NIA_SL_WORD0001_REAL02 ì¶”ì¶œ\n",
        "    name = os.path.basename(filename).replace(\"_morpheme.json\", \"\")\n",
        "    return name.rsplit(\"_\", 1)[0]\n",
        "\n",
        "for f in tqdm(morpheme_files, desc=\"Morpheme íŒŒì¼ ë¡œë“œ\"):\n",
        "    real_id = extract_real_id_from_morpheme(f)\n",
        "    base_word = real_id.split(\"_REAL\")[0] # NIA_SL_WORD0001\n",
        "\n",
        "    # ì„ íƒëœ ë‹¨ì–´ì— í¬í•¨ë˜ëŠ” ê²½ìš°ì—ë§Œ ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€\n",
        "    if base_word in selected_words:\n",
        "        # REAL IDë¥¼ ê³ ìœ  í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ 1:1 ë§¤ì¹­ ì¤€ë¹„\n",
        "        morpheme_dict[real_id] = f\n",
        "\n",
        "print(f\"í•„í„°ë§ëœ morpheme (REAL ID ê¸°ì¤€) ê°œìˆ˜: {len(morpheme_dict)}ê°œ\")\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 3. keypoint í´ë” ë¡œë“œ (F ë²„ì „ë§Œ)\n",
        "# ============================\n",
        "keypoint_dirs = []\n",
        "\n",
        "for root, dirs, files in os.walk(keypoint_root):\n",
        "    for d in dirs:\n",
        "        # ì˜ˆ: NIA_SL_WORD0001_REAL03_F í˜•ì‹ì˜ í´ë”ë§Œ ì„ íƒ\n",
        "        if d.startswith(\"NIA_SL_WORD\") and \"_REAL\" in d and d.endswith(\"_F\"):\n",
        "            keypoint_dirs.append(os.path.join(root, d))\n",
        "\n",
        "print(f\"ì„ íƒëœ keypoint í´ë” ìˆ˜: {len(keypoint_dirs)}ê°œ\")\n",
        "\n",
        "\n",
        "def extract_real_id_from_kp(folder_name):\n",
        "    # NIA_SL_WORD0001_REAL03_F â†’ NIA_SL_WORD0001_REAL03 ì¶”ì¶œ\n",
        "    return folder_name.rsplit(\"_\", 1)[0]\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 4. keypoint â†” morpheme 1:1 ë§¤ì¹­ ë° fast_index ìƒì„±\n",
        "# ============================\n",
        "fast_index = []\n",
        "\n",
        "for kp_path in tqdm(keypoint_dirs, desc=\"ë‹¨ì–´ ë§¤ì¹­ ì¤‘\"):\n",
        "    folder_name = os.path.basename(kp_path)\n",
        "\n",
        "    # 1:1 ë§¤ì¹­ì„ ìœ„í•œ REAL ID ì¶”ì¶œ\n",
        "    real_id = extract_real_id_from_kp(folder_name)\n",
        "\n",
        "    # base_word ì¶”ì¶œ (ì„ íƒëœ ë‹¨ì–´ í™•ì¸ìš©)\n",
        "    base_word = real_id.split(\"_REAL\")[0]\n",
        "\n",
        "    # base_wordê°€ ì„ íƒ ëª©ë¡ì— ìˆê³ , í•´ë‹¹ REAL IDì˜ morpheme íŒŒì¼ì´ morpheme_dictì— ìˆëŠ” ê²½ìš° ë§¤ì¹­\n",
        "    if base_word in selected_words and real_id in morpheme_dict:\n",
        "        morpheme_path = morpheme_dict[real_id]\n",
        "\n",
        "        fast_index.append({\n",
        "            \"word_id\": base_word,\n",
        "            \"real_id\": real_id, # ë§¤ì¹­ëœ ê³ ìœ  ID\n",
        "            \"keypoint_path\": kp_path,\n",
        "            \"morpheme_path\": morpheme_path\n",
        "        })\n",
        "\n",
        "print(f\"ìµœì¢… ë§¤ì¹­ëœ ë°ì´í„° ê°œìˆ˜: {len(fast_index)}ê°œ\")\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 5. JSON ì €ì¥\n",
        "# ============================\n",
        "output_path = os.path.join(output_dir, \"fast_match.json\")\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(fast_index, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(\"\\n ì €ì¥ ì™„ë£Œ:\", output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e_IrMCVWhLj",
        "outputId": "809c10b0-6006-4c4a-fa02-4550c2ed42c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ujson\n",
            "  Downloading ujson-5.11.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (9.4 kB)\n",
            "Downloading ujson-5.11.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (57 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/57.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ujson\n",
            "Successfully installed ujson-5.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ujson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfGGReu7CzSl",
        "outputId": "aef7b0d9-481d-429a-ff80-bbcbca3ddc56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì²˜ë¦¬í•  ë§¤ì¹­ ìŒ ê°œìˆ˜: 1760ê°œ\n",
            "ì‚¬ìš© ê°€ëŠ¥í•œ CPU ì½”ì–´ ìˆ˜: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ë°ì´í„° ê²°í•© ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1760/1760 [10:22:26<00:00, 21.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ ì¤‘...\n",
            "\n",
            "âœ… ì™„ë£Œ\n",
            "ìµœì¢… ì €ì¥ëœ í˜•íƒœì†Œ ì„¸ê·¸ë¨¼íŠ¸ ê°œìˆ˜: 1757ê°œ\n",
            "ê²½ë¡œ: /content/drive/MyDrive/sign_data/processed/merged_dataset.json\n"
          ]
        }
      ],
      "source": [
        "import os, json, ujson, numpy as np\n",
        "from tqdm import tqdm\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "# ============================\n",
        "# 0. ê²½ë¡œ ì„¤ì •\n",
        "# ============================\n",
        "# ì…ë ¥ íŒŒì¼: ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ 1:1 ë§¤ì¹­ ì¸ë±ìŠ¤ íŒŒì¼\n",
        "fast_index_path = \"/content/drive/MyDrive/sign_data/fast_index/fast_match.json\"\n",
        "# ì¶œë ¥ íŒŒì¼: Keypointì™€ í˜•íƒœì†Œ ì •ë³´ë¥¼ ê²°í•©í•œ ìµœì¢… ë°ì´í„°ì…‹\n",
        "output_path = \"/content/drive/MyDrive/sign_data/processed/merged_dataset.json\"\n",
        "\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "# ë§¤ì¹­ ì¸ë±ìŠ¤ ë¡œë“œ\n",
        "try:\n",
        "    with open(fast_index_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        pairs = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ì˜¤ë¥˜: fast_index íŒŒì¼ì´ ê²½ë¡œì— ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {fast_index_path}\")\n",
        "    pairs = []\n",
        "except Exception as e:\n",
        "    print(f\"ì˜¤ë¥˜: fast_index íŒŒì¼ ë¡œë“œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}\")\n",
        "    pairs = []\n",
        "\n",
        "print(f\"ì²˜ë¦¬í•  ë§¤ì¹­ ìŒ ê°œìˆ˜: {len(pairs)}ê°œ\")\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 1. Keypoint ë°ì´í„° ë¡œë“œ ë° ì •ì œ í•¨ìˆ˜\n",
        "# ============================\n",
        "def load_person_data(fp):\n",
        "    \"\"\"í”„ë ˆì„ í•˜ë‚˜ ë¹ ë¥´ê²Œ ì½ê³  (N, 3) Keypoint ë°°ì—´ ë°˜í™˜\"\"\"\n",
        "    try:\n",
        "        # ujsonìœ¼ë¡œ íŒŒì¼ ë¹ ë¥´ê²Œ ë¡œë“œ\n",
        "        with open(fp, \"r\") as f:\n",
        "            kf = ujson.load(f)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "    # 'people' ë°ì´í„° ì¶”ì¶œ ë° í˜•ì‹ í†µì¼\n",
        "    people = kf.get(\"people\", [])\n",
        "    if isinstance(people, dict):\n",
        "        people = [people.get(\"0\", people)]\n",
        "\n",
        "    if not people:\n",
        "        return None\n",
        "\n",
        "    person = people[0]\n",
        "\n",
        "    def safe_array(key):\n",
        "        \"\"\"í‚¤í¬ì¸íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ê³  (N, 3) í˜•íƒœë¡œ ë³€í™˜\"\"\"\n",
        "        arr = person.get(key, [])\n",
        "        if not arr:\n",
        "            # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° (0, 3) ë°°ì—´ ë°˜í™˜\n",
        "            return np.zeros((0, 3), dtype=np.float32)\n",
        "\n",
        "        arr = np.array(arr, dtype=np.float32)\n",
        "        # 2D í‚¤í¬ì¸íŠ¸ëŠ” (N*3,) í˜•íƒœì´ë¯€ë¡œ (N, 3)ìœ¼ë¡œ ì¬êµ¬ì„±\n",
        "        return arr.reshape(-1, 3)\n",
        "\n",
        "    # Keypoint ì¶”ì¶œ ë° ì—°ê²° ìˆœì„œ: Pose(25) + LeftHand(21) + RightHand(21) + Face(70) = 137 Keypoints\n",
        "    p = safe_array(\"pose_keypoints_2d\")\n",
        "    lh = safe_array(\"hand_left_keypoints_2d\")\n",
        "    rh = safe_array(\"hand_right_keypoints_2d\")\n",
        "    face = safe_array(\"face_keypoints_2d\")\n",
        "\n",
        "    # ëª¨ë“  í‚¤í¬ì¸íŠ¸ ì—°ê²°\n",
        "    full = np.concatenate([p, lh, rh, face], axis=0)\n",
        "\n",
        "    # ì‹ ë¢°ë„(Confidence, 3ë²ˆì§¸ ì—´)ê°€ 0.5 ë¯¸ë§Œì¸ í‚¤í¬ì¸íŠ¸ëŠ” 0ìœ¼ë¡œ ì„¤ì •\n",
        "    full[full[:, 2] < 0.5] = 0\n",
        "\n",
        "    return full\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 2. ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜\n",
        "# ============================\n",
        "def process_item(item):\n",
        "    \"\"\"ë‹¨ì–´ 1ê°œì˜ Keypointì™€ Morphemeì„ ê²°í•©í•˜ì—¬ í”„ë ˆì„ ì‹œí€€ìŠ¤ ìƒì„±\"\"\"\n",
        "    keypoint_dir = item[\"keypoint_path\"]\n",
        "    morpheme_path = item[\"morpheme_path\"]\n",
        "    fps = 30 # ì´ˆë‹¹ í”„ë ˆì„ ìˆ˜ (NIA ìˆ˜ì–´ ë°ì´í„°ì…‹ í‘œì¤€)\n",
        "    real_id = item.get(\"real_id\", \"N/A\") # real_id ê°€ì ¸ì˜¤ê¸°\n",
        "\n",
        "    # Morpheme ë¡œë“œ\n",
        "    try:\n",
        "        with open(morpheme_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            mor = ujson.load(f)\n",
        "        segments = mor.get(\"data\", [])\n",
        "        if not isinstance(segments, list):\n",
        "             segments = [segments]\n",
        "    except Exception:\n",
        "        # print(f\"Morpheme ë¡œë“œ ì˜¤ë¥˜: {morpheme_path}\")\n",
        "        return []\n",
        "\n",
        "    # Keypoint í”„ë ˆì„ íŒŒì¼ ëª©ë¡\n",
        "    frame_files = sorted([\n",
        "        os.path.join(keypoint_dir, f)\n",
        "        for f in os.listdir(keypoint_dir)\n",
        "        if f.endswith(\".json\")\n",
        "    ])\n",
        "\n",
        "    if not frame_files:\n",
        "        return []\n",
        "\n",
        "    # ëª¨ë“  í”„ë ˆì„ ë°ì´í„° ë¡œë“œ\n",
        "    frames = []\n",
        "    for fp in frame_files:\n",
        "        full = load_person_data(fp)\n",
        "        # í‚¤í¬ì¸íŠ¸ ê°œìˆ˜ í™•ì¸ (137ê°œ = 25+21+21+70)\n",
        "        if full is not None and full.shape[0] == 137:\n",
        "            frames.append(full)\n",
        "\n",
        "    if not frames:\n",
        "        return []\n",
        "\n",
        "    frames = np.stack(frames).astype(np.float32)\n",
        "    results = []\n",
        "\n",
        "    # í˜•íƒœì†Œ êµ¬ê°„ë³„ ì˜ë¼ì„œ ì €ì¥\n",
        "    for seg in segments:\n",
        "        try:\n",
        "            start_idx = int(seg[\"start\"] * fps)\n",
        "            end_idx = int(seg[\"end\"] * fps)\n",
        "\n",
        "            # ìœ íš¨ì„± ê²€ì‚¬\n",
        "            if start_idx >= len(frames) or end_idx <= start_idx:\n",
        "                continue\n",
        "\n",
        "            # í”„ë ˆì„ ìë¥´ê¸°\n",
        "            cut = frames[start_idx:min(end_idx, len(frames))]\n",
        "\n",
        "            results.append({\n",
        "                \"word_id\": item[\"word_id\"],\n",
        "                \"real_id\": real_id,\n",
        "                \"label\": seg[\"attributes\"], # í˜•íƒœì†Œ ë ˆì´ë¸”\n",
        "                \"frames\": cut.tolist() # ë„˜íŒŒì´ ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
        "            })\n",
        "        except Exception:\n",
        "             # print(f\"ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {real_id} - {seg}\")\n",
        "             continue\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 3. ë‹¨ì–´ ì „ì²´ë¥¼ ë³‘ë ¬ ì²˜ë¦¬ ë° ë°ì´í„° ë³‘í•©\n",
        "# ============================\n",
        "merged_data = []\n",
        "if pairs:\n",
        "    # CPU ì½”ì–´ ìˆ˜ë§Œí¼ Pool ìƒì„±\n",
        "    core_count = cpu_count()\n",
        "    print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ CPU ì½”ì–´ ìˆ˜: {core_count}\")\n",
        "    with Pool(core_count) as P:\n",
        "        # imap_unorderedë¥¼ ì‚¬ìš©í•˜ì—¬ ê²°ê³¼ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ê°€ì ¸ì˜´\n",
        "        for res in tqdm(P.imap_unordered(process_item, pairs), total=len(pairs), desc=\"ë°ì´í„° ê²°í•© ì¤‘\"):\n",
        "            merged_data.extend(res)\n",
        "\n",
        "# ============================\n",
        "# 4. JSON ì €ì¥ (ujson ì‚¬ìš©)\n",
        "# ============================\n",
        "if merged_data:\n",
        "    try:\n",
        "        print(\"\\në°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ ì¤‘...\")\n",
        "        with open(output_path, \"w\") as f:\n",
        "            ujson.dump(merged_data, f)\n",
        "\n",
        "        print(\"\\n ì™„ë£Œ\")\n",
        "        print(f\"ìµœì¢… ì €ì¥ëœ í˜•íƒœì†Œ ì„¸ê·¸ë¨¼íŠ¸ ê°œìˆ˜: {len(merged_data)}ê°œ\")\n",
        "        print(f\"ê²½ë¡œ: {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
        "else:\n",
        "    print(\"\\n ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ì–´ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0dlhcYg2zXOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. íŒŒì¼ ë¡œë“œ\n",
        "file_path = '/content/drive/MyDrive/sign_data/processed/merged_dataset.json'\n",
        "\n",
        "print(\"ë°ì´í„° ë¡œë“œ...\")\n",
        "with open(file_path, 'r') as f:\n",
        "    data_json = json.load(f)\n",
        "print(\"ë¡œë“œ ì™„ë£Œ. ì´ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜:\", len(data_json))\n",
        "\n",
        "\n",
        "# 2. frames padding â†’ raw_data ìƒì„±\n",
        "keypoint_data_list = [item[\"frames\"] for item in data_json]\n",
        "\n",
        "print(\"padding ì¤‘...\")\n",
        "padded_data = pad_sequences(keypoint_data_list, padding='post', dtype='float32')\n",
        "raw_data = np.array(padded_data)\n",
        "print(\"raw_data shape:\", raw_data.shape)   # (N, T, 137, 3)\n",
        "\n",
        "\n",
        "# 3. 4D â†’ (N,T,F) reshape\n",
        "N, T, P, C = raw_data.shape\n",
        "F = P * C\n",
        "raw_flat = raw_data.reshape(N, T, F)\n",
        "print(\"reshape ì™„ë£Œ:\", raw_flat.shape)\n",
        "\n",
        "\n",
        "# 4. Mid-Hip ìƒëŒ€ì¢Œí‘œ (x,yë§Œ ë³€í™˜)\n",
        "def apply_midhip_relative(raw):\n",
        "    print(\"Mid-Hip ìƒëŒ€ì¢Œí‘œ ë³€í™˜ ì¤‘...\")\n",
        "\n",
        "    N, T, F = raw.shape\n",
        "    num_points = F // 3\n",
        "\n",
        "    arr = raw.reshape(N, T, num_points, 3)\n",
        "\n",
        "    REF = 0  # Mid-Hip index\n",
        "    ref_xy = arr[:, :, REF, :2]\n",
        "    ref_xy = ref_xy[:, :, None, :]\n",
        "\n",
        "    arr[:, :, :, :2] -= ref_xy  # x,yë§Œ ìƒëŒ€ì¢Œí‘œ\n",
        "\n",
        "    return arr.reshape(N, T, F)\n",
        "\n",
        "X_midhip = apply_midhip_relative(raw_flat)\n",
        "print(\"Mid-Hip ì™„ë£Œ:\", X_midhip.shape)\n",
        "\n",
        "\n",
        "# 5. í•µì‹¬ keypoint ì„ íƒ (18ê°œ â†’ 54ì°¨ì›)\n",
        "POSE_IDXS = [0,1,2,3,4,5,6,7]\n",
        "HAND_IDXS = [4,8,12,16,20]\n",
        "\n",
        "def select_keypoints(frame_137x3):\n",
        "    pose = frame_137x3[:25]\n",
        "    lh   = frame_137x3[25:46]\n",
        "    rh   = frame_137x3[46:67]\n",
        "\n",
        "    selected = []\n",
        "    for idx in POSE_IDXS: selected.append(pose[idx])\n",
        "    for idx in HAND_IDXS: selected.append(lh[idx])\n",
        "    for idx in HAND_IDXS: selected.append(rh[idx])\n",
        "\n",
        "    return np.array(selected).reshape(-1)\n",
        "\n",
        "def apply_keypoint_selection(X):\n",
        "    print(\"í•µì‹¬ keypoint ì„ íƒ ì¤‘...\")\n",
        "    N,T,F = X.shape\n",
        "    num_points = F//3\n",
        "    Xr = X.reshape(N,T,num_points,3)\n",
        "\n",
        "    X_new = np.zeros((N,T,54), dtype=np.float32)\n",
        "\n",
        "    for i in range(N):\n",
        "        for t in range(T):\n",
        "            X_new[i,t] = select_keypoints(Xr[i,t])\n",
        "\n",
        "    print(\"ì™„ë£Œ:\", X_new.shape)\n",
        "    return X_new\n",
        "\n",
        "X_selected = apply_keypoint_selection(X_midhip)\n",
        "\n",
        "\n",
        "# â­â­â­ ê°•í™”ëœ interpolation (v2) â­â­â­\n",
        "def interpolate_strong(frames, th=0.5):\n",
        "    \"\"\"\n",
        "    v1 â†’ prev/next ê¸°ë°˜ ë³´ê°„\n",
        "    v2 â†’ forward fill + backward fill + segment linear interpolation ì¶”ê°€\n",
        "    \"\"\"\n",
        "    T,F = frames.shape\n",
        "    P = F//3\n",
        "    arr = frames.reshape(T, P, 3)\n",
        "\n",
        "    for p in range(P):  # ê° keypointë§ˆë‹¤ ì²˜ë¦¬\n",
        "        conf = arr[:, p, 2]\n",
        "        x = arr[:, p, 0]\n",
        "        y = arr[:, p, 1]\n",
        "\n",
        "        # ---- 1) forward fill (ë’¤ì—ì„œ ê°€ì ¸ì™€ ì±„ìš°ê¸°) ----\n",
        "        for t in range(1, T):\n",
        "            if conf[t] < th and conf[t-1] >= th:\n",
        "                x[t] = x[t-1]\n",
        "                y[t] = y[t-1]\n",
        "                conf[t] = conf[t-1]\n",
        "\n",
        "        # ---- 2) backward fill (ì•ì—ì„œ ê°€ì ¸ì™€ ì±„ìš°ê¸°) ----\n",
        "        for t in range(T-2, -1, -1):\n",
        "            if conf[t] < th and conf[t+1] >= th:\n",
        "                x[t] = x[t+1]\n",
        "                y[t] = y[t+1]\n",
        "                conf[t] = conf[t+1]\n",
        "\n",
        "        # ---- 3) segment-based interpolation ----\n",
        "        low_idx = np.where(conf < th)[0]\n",
        "\n",
        "        if len(low_idx) > 0:\n",
        "            # ì—°ì† êµ¬ê°„ íƒìƒ‰\n",
        "            seg_starts = []\n",
        "            seg_ends = []\n",
        "\n",
        "            start = low_idx[0]\n",
        "            prev = low_idx[0]\n",
        "\n",
        "            for idx in low_idx[1:]:\n",
        "                if idx != prev + 1:\n",
        "                    seg_starts.append(start)\n",
        "                    seg_ends.append(prev)\n",
        "                    start = idx\n",
        "                prev = idx\n",
        "            seg_starts.append(start)\n",
        "            seg_ends.append(prev)\n",
        "\n",
        "            # ê° ì—°ì† êµ¬ê°„ ë³´ê°„\n",
        "            for s, e in zip(seg_starts, seg_ends):\n",
        "                prev_t = s - 1\n",
        "                next_t = e + 1\n",
        "\n",
        "                if prev_t < 0 or next_t >= T:\n",
        "                    continue\n",
        "\n",
        "                # ì„ í˜• ë³´ê°„\n",
        "                for t in range(s, e+1):\n",
        "                    ratio = (t - s + 1) / (e - s + 2)\n",
        "                    x[t] = (1-ratio) * x[prev_t] + ratio * x[next_t]\n",
        "                    y[t] = (1-ratio) * y[prev_t] + ratio * y[next_t]\n",
        "                    conf[t] = max(conf[prev_t], conf[next_t])\n",
        "\n",
        "        arr[:, p, 0] = x\n",
        "        arr[:, p, 1] = y\n",
        "        arr[:, p, 2] = conf\n",
        "\n",
        "    return arr.reshape(T, F)\n",
        "\n",
        "\n",
        "print(\"ê°•í™”ëœ interpolation v2 ì ìš© ì¤‘...\")\n",
        "X_interp = np.zeros_like(X_selected)\n",
        "for i in range(len(X_selected)):\n",
        "    X_interp[i] = interpolate_strong(X_selected[i])\n",
        "print(\"ë³´ê°„ ì™„ë£Œ:\", X_interp.shape)\n",
        "\n",
        "\n",
        "# 7. confidence ì œê±° (x,yë§Œ)\n",
        "def remove_confidence(X):\n",
        "    N,T,F = X.shape\n",
        "    P = F//3\n",
        "    coords = X.reshape(N,T,P,3)[:,:,:,:2]\n",
        "    return coords.reshape(N,T,P*2)\n",
        "\n",
        "X_xy = remove_confidence(X_interp)\n",
        "print(\"confidence ì œê±° ì™„ë£Œ:\", X_xy.shape)\n",
        "\n",
        "\n",
        "# 8. ë‹¨ìˆœ ìŠ¤ì¼€ì¼ë§\n",
        "X_xy /= 1000.0\n",
        "\n",
        "\n",
        "# 9. Train/Test split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "labels_text = np.array([item[\"label\"][0][\"name\"] for item in data_json])\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(labels_text)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_xy, y,\n",
        "    test_size=0.2,\n",
        "    shuffle=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train:\", X_train.shape)\n",
        "print(\"Test :\", X_test.shape)\n",
        "\n",
        "print(\"ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ v\")\n"
      ],
      "metadata": {
        "id": "Nv0Vd1eLzXuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QKDVTFFjnlTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I62Fesbs6Uvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 0. í™˜ê²½ ì´ˆê¸°í™” & ê¸°ë³¸ ì„¤ì •\n",
        "# ============================================\n",
        "import gc\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Masking, Bidirectional, GRU,\n",
        "    Dense, Dropout, LayerNormalization,\n",
        "    Attention\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# ë©”ëª¨ë¦¬ / ì„¸ì…˜ ì´ˆê¸°í™”\n",
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# ì¬í˜„ì„±\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"âœ”ï¸ Environment reset complete.\")\n",
        "\n",
        "# ============================================\n",
        "# 1. ë°ì´í„° ë¡œë“œ & ë¼ë²¨ ì •ì œ\n",
        "# ============================================\n",
        "file_path = '/content/drive/MyDrive/sign_data/processed/merged_dataset.json'\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    data_json = json.load(f)\n",
        "\n",
        "print(\"ğŸ“¦ ì›ë³¸ ìƒ˜í”Œ ìˆ˜:\", len(data_json))\n",
        "\n",
        "# í•„ìˆ˜ í‚¤ ì²´í¬\n",
        "required_keys = {\"frames\", \"label\"}\n",
        "issues = [i for i, item in enumerate(data_json) if not required_keys.issubset(item.keys())]\n",
        "print(\"âš ï¸ êµ¬ì¡° ì´ìƒ ìƒ˜í”Œ ìˆ˜:\", len(issues))\n",
        "\n",
        "# ë¼ë²¨ ì˜¤íƒ€ â†’ í‘œì¤€ ë¼ë²¨ ë§¤í•‘\n",
        "label_fix_map = {\n",
        "    \"ê½ˆë² ê¸°\": \"ê½ˆë°°ê¸°\",\n",
        "    \"ë°°ì¶”êµ­\": \"ë°°ì¶§êµ­\",\n",
        "    \"ëœì¥ì°Œê²Œ\": \"ëœì¥ì°Œê°œ\",\n",
        "}\n",
        "\n",
        "for item in data_json:\n",
        "    label = item['label'][0]['name']\n",
        "    if label in label_fix_map:\n",
        "        item['label'][0]['name'] = label_fix_map[label]\n",
        "\n",
        "# ë¼ë²¨ ë¶„í¬ í™•ì¸\n",
        "labels_all = [item['label'][0]['name'] for item in data_json]\n",
        "label_counts_all = Counter(labels_all)\n",
        "print(\"ğŸ¯ ì›ë³¸ í´ë˜ìŠ¤ ê°œìˆ˜:\", len(label_counts_all))\n",
        "\n",
        "# ìƒ˜í”Œ 2ê°œ ë¯¸ë§Œ ë¼ë²¨ ì œê±°\n",
        "remove_labels = [k for k, v in label_counts_all.items() if v < 2]\n",
        "filtered_data = [item for item in data_json if item['label'][0]['name'] not in remove_labels]\n",
        "\n",
        "print(\"ğŸ§¹ ë¼ë²¨ ì •ì œ ì™„ë£Œ!\")\n",
        "print(\"ğŸ“¦ ì •ì œ í›„ ìƒ˜í”Œ ìˆ˜:\", len(filtered_data))\n",
        "print(\"ğŸ—‘ ì œê±°ëœ ë¼ë²¨:\", remove_labels)\n",
        "\n",
        "# ì •ì œ í›„ ë¼ë²¨ ë¶„í¬\n",
        "labels_filtered = [item['label'][0]['name'] for item in filtered_data]\n",
        "label_counts_filtered = Counter(labels_filtered)\n",
        "print(\"ğŸ¯ ì •ì œ í›„ í´ë˜ìŠ¤ ê°œìˆ˜:\", len(label_counts_filtered))\n",
        "print(\"ğŸ” ìƒìœ„ 10ê°œ:\", label_counts_filtered.most_common(10))\n",
        "print(\"ğŸ“‰ ìƒ˜í”Œìˆ˜ 2 ë¯¸ë§Œ ì—¬ë¶€:\", [k for k,v in label_counts_filtered.items() if v < 2])\n",
        "\n",
        "# ê° ìƒ˜í”Œ ê¸¸ì´\n",
        "seq_lengths_all = np.array([len(item['frames']) for item in filtered_data])\n",
        "print(\"ğŸ“ ìµœì†Œ ê¸¸ì´:\", seq_lengths_all.min())\n",
        "print(\"ğŸ“ ìµœëŒ€ ê¸¸ì´:\", seq_lengths_all.max())\n",
        "print(\"ğŸ“ í‰ê·  ê¸¸ì´:\", seq_lengths_all.mean())\n",
        "\n",
        "plt.hist(seq_lengths_all, bins=30)\n",
        "plt.title(\"Sequence Length Distribution (filtered)\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# 2. frames padding â†’ (N, T, 137, 3)\n",
        "# ============================================\n",
        "def pad_frames(dataset):\n",
        "    sequences = [item[\"frames\"] for item in dataset]\n",
        "    padded = pad_sequences(sequences, padding='post', dtype='float32')\n",
        "    print(\"ğŸ“Œ Padding ì™„ë£Œ:\", padded.shape)\n",
        "    return padded\n",
        "\n",
        "X_raw = pad_frames(filtered_data)   # (N, T, 137, 3)\n",
        "N, T_max, P, C = X_raw.shape\n",
        "print(\"X_raw shape:\", X_raw.shape)\n",
        "\n",
        "# ============================================\n",
        "# 3. Mid-Hip ê¸°ì¤€ ìƒëŒ€ ì¢Œí‘œ ë³€í™˜\n",
        "# ============================================\n",
        "def normalize_midhip(X):\n",
        "    \"\"\"\n",
        "    X : (N, T, P, C=3)\n",
        "    Mid-Hip(0ë²ˆ í¬ì¸íŠ¸)ì„ ê¸°ì¤€ìœ¼ë¡œ x,y ìƒëŒ€ì¢Œí‘œë¡œ ë³€í™˜\n",
        "    \"\"\"\n",
        "    X = X.copy()\n",
        "    N, T, P, C = X.shape\n",
        "\n",
        "    REF_INDEX = 0  # Mid-Hip\n",
        "    ref_xy = X[:, :, REF_INDEX, :2]         # (N,T,2)\n",
        "    ref_xy = ref_xy[:, :, None, :]          # (N,T,1,2)\n",
        "\n",
        "    X[:, :, :, :2] -= ref_xy                # x,y â†’ ìƒëŒ€ì¢Œí‘œ\n",
        "\n",
        "    print(\"ğŸ§­ Mid-Hip Normalize ì™„ë£Œ:\", X.shape)\n",
        "    return X\n",
        "\n",
        "X_norm_mhip = normalize_midhip(X_raw)\n",
        "\n",
        "# ============================================\n",
        "# 4. í¬ì¦ˆ + ì–‘ì† ìŠ¤ì¼ˆë ˆí†¤ XYë§Œ ì¶”ì¶œ â†’ (N, T, D)\n",
        "#    (ëª¸ 25 + ì™¼ì† 21 + ì˜¤ë¥¸ì† 21 = 67 í¬ì¸íŠ¸)\n",
        "# ============================================\n",
        "def extract_xy_features(X):\n",
        "    \"\"\"\n",
        "    X : (N, T, 137, 3)\n",
        "    ë°˜í™˜: (N, T, 67*2=134)  â†’ pose + left hand + right hand, x,yë§Œ\n",
        "    \"\"\"\n",
        "    N, T, P, C = X.shape\n",
        "\n",
        "    pose = X[:, :, :25, :]      # body\n",
        "    lh   = X[:, :, 25:46, :]    # left hand\n",
        "    rh   = X[:, :, 46:67, :]    # right hand\n",
        "\n",
        "    full = np.concatenate([pose, lh, rh], axis=2)  # (N,T,67,3)\n",
        "\n",
        "    xy = full[:, :, :, :2]      # x,yë§Œ\n",
        "    final = xy.reshape(N, T, -1)\n",
        "\n",
        "    print(\"ğŸ§¬ Full Skeleton XY Features:\", final.shape)\n",
        "    return final\n",
        "\n",
        "X_xy_all = extract_xy_features(X_norm_mhip)   # (N, T, 134)\n",
        "\n",
        "# ============================================\n",
        "# 5. ë¼ë²¨ ì¸ì½”ë”© & Train/Test Split (stratify)\n",
        "# ============================================\n",
        "encoder = LabelEncoder()\n",
        "y_all = encoder.fit_transform(labels_filtered)\n",
        "num_classes = len(np.unique(y_all))\n",
        "print(\"ğŸ¯ í´ë˜ìŠ¤ ìˆ˜:\", num_classes)\n",
        "\n",
        "idx_all = np.arange(len(y_all))\n",
        "idx_train, idx_test = train_test_split(\n",
        "    idx_all,\n",
        "    test_size=0.2,\n",
        "    shuffle=True,\n",
        "    stratify=y_all,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train_raw = X_xy_all[idx_train]\n",
        "X_test_raw  = X_xy_all[idx_test]\n",
        "y_train = y_all[idx_train]\n",
        "y_test  = y_all[idx_test]\n",
        "seq_train = seq_lengths_all[idx_train]\n",
        "seq_test  = seq_lengths_all[idx_test]\n",
        "\n",
        "print(\"ğŸ“¦ Train Raw:\", X_train_raw.shape)\n",
        "print(\"ğŸ“¦ Test  Raw:\", X_test_raw.shape)\n",
        "\n",
        "# ============================================\n",
        "# 6. Z-Score ì •ê·œí™” (Train ìœ íš¨ í”„ë ˆì„ ê¸°ì¤€)\n",
        "#    â†’ íŒ¨ë”©(ê¸¸ì´ ì´í›„)ì€ ê·¸ëŒ€ë¡œ 0 ìœ ì§€\n",
        "# ============================================\n",
        "def compute_mean_std_nonpad(X, seq_lengths):\n",
        "    \"\"\"\n",
        "    X : (N,T,D), seq_lengths : (N,)\n",
        "    ìœ íš¨ í”„ë ˆì„(t < seq_len)ì— ëŒ€í•´ì„œë§Œ mean, std ê³„ì‚°\n",
        "    \"\"\"\n",
        "    N, T, D = X.shape\n",
        "    total_frames = int(seq_lengths.sum())\n",
        "\n",
        "    s = np.zeros(D, dtype=np.float64)\n",
        "    ss = np.zeros(D, dtype=np.float64)\n",
        "\n",
        "    for i in range(N):\n",
        "        L = seq_lengths[i]\n",
        "        if L == 0:\n",
        "            continue\n",
        "        Xi = X[i, :L, :]       # (L,D)\n",
        "        s  += Xi.sum(axis=0)\n",
        "        ss += (Xi ** 2).sum(axis=0)\n",
        "\n",
        "    mean = s / total_frames\n",
        "    var  = ss / total_frames - mean**2\n",
        "    std  = np.sqrt(var + 1e-6)\n",
        "\n",
        "    return mean.astype(np.float32), std.astype(np.float32)\n",
        "\n",
        "mean_vec, std_vec = compute_mean_std_nonpad(X_train_raw, seq_train)\n",
        "print(\"ğŸ“Š mean shape:\", mean_vec.shape, \"std shape:\", std_vec.shape)\n",
        "\n",
        "def apply_norm_nonpad(X, seq_lengths, mean, std):\n",
        "    \"\"\"\n",
        "    X : (N,T,D)\n",
        "    ê° ìƒ˜í”Œì˜ ìœ íš¨ í”„ë ˆì„(t < seq_len)ì—ë§Œ ì •ê·œí™” ì ìš©\n",
        "    ë‚˜ë¨¸ì§€ íŒ¨ë”© êµ¬ê°„ì€ 0 ìœ ì§€\n",
        "    \"\"\"\n",
        "    Xn = np.zeros_like(X, dtype=np.float32)\n",
        "    N, T, D = X.shape\n",
        "\n",
        "    for i in range(N):\n",
        "        L = seq_lengths[i]\n",
        "        if L == 0:\n",
        "            continue\n",
        "        Xi = X[i, :L, :]\n",
        "        Xn[i, :L, :] = (Xi - mean) / std\n",
        "        # ë‚˜ë¨¸ì§€ [L:]ëŠ” 0 (íŒ¨ë”©)\n",
        "\n",
        "    return Xn\n",
        "\n",
        "X_train = apply_norm_nonpad(X_train_raw, seq_train, mean_vec, std_vec)\n",
        "X_test  = apply_norm_nonpad(X_test_raw,  seq_test,  mean_vec, std_vec)\n",
        "\n",
        "print(\"ğŸ§ª Normalization ì™„ë£Œ.\")\n",
        "print(\"ğŸ“¦ Train:\", X_train.shape)\n",
        "print(\"ğŸ“¦ Test :\", X_test.shape)\n",
        "\n",
        "# ============================================\n",
        "# 7. ë°ì´í„° ì¦ê°• (Trainë§Œ / Time Stretch ì œê±°)\n",
        "#    - Gaussian Noise\n",
        "#    - Frame Dropout\n",
        "# ============================================\n",
        "def augment_data_v2(x, seq_len,\n",
        "                    noise_std=0.01,\n",
        "                    dropout_rate=0.05,\n",
        "                    shift_range=0.05,\n",
        "                    scale_range=0.1):\n",
        "    \"\"\"\n",
        "    x        : (T_max, D)\n",
        "    seq_len  : ì‹¤ì œ ìœ íš¨ ê¸¸ì´\n",
        "    - ì¢Œ/ìš° ë°˜ì „ ì—†ìŒ\n",
        "    - ìœ íš¨ êµ¬ê°„ë§Œ ì¦ê°•, íŒ¨ë”© êµ¬ê°„ì€ ê·¸ëŒ€ë¡œ 0 ìœ ì§€\n",
        "    - frame dropoutë„ 'ì™„ì „ 0'ì€ í”¼í•˜ê³ , ê±°ì˜ 0ì— ê°€ê¹ê²Œë§Œ ì¤„ì—¬ì„œ Maskingì— ì•ˆ ê±¸ë¦¬ê²Œ í•¨\n",
        "    \"\"\"\n",
        "    T_max, D = x.shape\n",
        "    L = int(seq_len)\n",
        "\n",
        "    if L <= 0:\n",
        "        return x.copy()\n",
        "\n",
        "    x_out = x.copy()\n",
        "    x_valid = x_out[:L]\n",
        "\n",
        "    # 1) Gaussian noise\n",
        "    x_valid += np.random.normal(0, noise_std, size=x_valid.shape).astype(x_valid.dtype)\n",
        "\n",
        "    # 2) global shift (ì „ì²´ ì¢Œí‘œ í‰í–‰ì´ë™)\n",
        "    shift = np.random.uniform(-shift_range, shift_range)\n",
        "    x_valid += shift\n",
        "\n",
        "    # 3) global scale (ëª¸ í¬ê¸°/ì¹´ë©”ë¼ ê±°ë¦¬ ë³€í™”)\n",
        "    scale = 1.0 + np.random.uniform(-scale_range, scale_range)\n",
        "    x_valid *= scale\n",
        "\n",
        "    # 4) frame dropout (ì™„ì „ 0ì´ ì•„ë‹ˆë¼ ì•½í•˜ê²Œ ì¤„ì´ê¸°)\n",
        "    drop_mask = np.random.rand(L) < dropout_rate\n",
        "    if drop_mask.any():\n",
        "        x_valid[drop_mask] *= 0.1  # ì—ë„ˆì§€ë§Œ ì¤„ì´ê³  0ì€ í”¼í•œë‹¤\n",
        "\n",
        "    x_out[:L] = x_valid\n",
        "    # [L:] êµ¬ê°„ì€ ì›ë˜ë¶€í„° 0 íŒ¨ë”© â†’ ê·¸ëŒ€ë¡œ ë‘ \n",
        "    return x_out\n",
        "\n",
        "X_aug_list = []\n",
        "y_aug_list = []\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "    X_aug_list.append(augment_data_v2(X_train[i], seq_train[i]))\n",
        "    y_aug_list.append(y_train[i])\n",
        "\n",
        "X_augmented = np.stack(X_aug_list)\n",
        "y_augmented = np.array(y_aug_list)\n",
        "\n",
        "X_train2 = np.concatenate([X_train, X_augmented], axis=0)\n",
        "y_train2 = np.concatenate([y_train, y_augmented], axis=0)\n",
        "\n",
        "print(\"ğŸ“¦ ì›ë³¸ Train:\", X_train.shape, len(y_train))\n",
        "print(\"ğŸ“¦ ì¦ê°• í¬í•¨ Train:\", X_train2.shape, len(y_train2))\n",
        "\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "\n",
        "def build_hybrid_model(input_dim, num_classes):\n",
        "    inputs = Input(shape=(None, input_dim))\n",
        "\n",
        "    # 0) Masking (ì˜¤ë¥¸ìª½ íŒ¨ë”© 0ë§Œ ë§ˆìŠ¤í¬)\n",
        "    x = Masking(mask_value=0.0)(inputs)\n",
        "\n",
        "    # 1) BiGRUë¡œ 1ì°¨ ì‹œí€€ìŠ¤ ì¸ì½”ë”©\n",
        "    x = Bidirectional(\n",
        "        GRU(\n",
        "            64,\n",
        "            return_sequences=True,\n",
        "            dropout=0.2,\n",
        "            reset_after=False,\n",
        "            use_cudnn=False\n",
        "        )\n",
        "    )(x)\n",
        "\n",
        "    # 2) Transformer Encoder ë¸”ë¡ 1ê°œ\n",
        "    #    - Multi-Head Self-Attention\n",
        "    attn_out = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n",
        "    x = LayerNormalization()(x + attn_out)\n",
        "\n",
        "    #    - Position-wise Feed Forward\n",
        "    ff = Dense(128, activation='relu')(x)\n",
        "    ff = Dropout(0.3)(ff)\n",
        "    x = LayerNormalization()(x + ff)\n",
        "\n",
        "    # 3) GRUë¡œ ìš”ì•½\n",
        "    x = GRU(\n",
        "        32,\n",
        "        dropout=0.3,\n",
        "        reset_after=False,\n",
        "        use_cudnn=False\n",
        "    )(x)\n",
        "\n",
        "    # 4) Dense head\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=3e-4),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 9. Class Weight ì ìš© (ë¶ˆê· í˜• ë°ì´í„° ëŒ€ì‘)\n",
        "# ============================================\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# class weight ì¬ê³„ì‚° (ì¦ê°• í¬í•¨ y_train2 ê¸°ì¤€)\n",
        "class_weights = compute_class_weight(\n",
        "    \"balanced\",\n",
        "    classes=np.unique(y_train2),\n",
        "    y=y_train2\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(\"\\nğŸ“Œ Class Weights:\", class_weights)\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 10. í•™ìŠµ\n",
        "# ============================================\n",
        "model = build_hybrid_model(X_train2.shape[-1], num_classes)\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, verbose=1)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train2, y_train2,\n",
        "    validation_split=0.2,\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "def plot_history(history, title=\"Model\"):\n",
        "    plt.figure(figsize=(12,5))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title(f'{title} - Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "    plt.title(f'{title} - Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def print_results(history):\n",
        "    best_epoch = int(np.argmin(history.history['val_loss']) + 1)\n",
        "    print(f\"\\nğŸ”¥ Best Epoch: {best_epoch}\")\n",
        "    print(f\"ğŸ“Œ Final Train Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
        "    print(f\"ğŸ“Œ Best Validation Accuracy: {history.history['val_accuracy'][best_epoch-1]:.4f}\")\n",
        "    print(f\"ğŸ“Œ Final Train Loss: {history.history['loss'][-1]:.4f}\")\n",
        "    print(f\"ğŸ“Œ Best Validation Loss: {history.history['val_loss'][best_epoch-1]:.4f}\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 11. ê²°ê³¼ ì¶œë ¥\n",
        "# ============================================\n",
        "plot_history(history)\n",
        "print_results(history)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nğŸ§ª Test Loss: {test_loss:.4f},  Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# (ì„ íƒ) ëª¨ë¸ ì €ì¥\n",
        "# model.save('/content/drive/MyDrive/sign_data/models/final_bigru_attention.h5')"
      ],
      "metadata": {
        "id": "TjeAHjPyRsxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8s6k51ufkHjC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}